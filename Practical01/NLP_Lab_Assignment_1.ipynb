{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56UoZrNvZDb4",
        "outputId": "0098079e-da4d-4fa4-c5f3-e70329871383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "# Install nltk library (only once per Colab session)\n",
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import nltk\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJMP3YQ5ZMOy",
        "outputId": "4426fe71-76b3-4e68-eb1d-63e89d50fcdc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text used for all techniques\n",
        "text = \"I don't like bad design! Natural language processing is amazing. \\\n",
        "Check this out @OpenAI #NLP #AI \"\n"
      ],
      "metadata": {
        "id": "YZTVQ4V3ZRnI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸŸ¢ PART 1: TOKENIZATION**"
      ],
      "metadata": {
        "id": "EzONmok7axFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Whitespace Tokenization\n",
        "whitespace_tokens = text.split()\n",
        "\n",
        "print(\"Whitespace Tokenization:\")\n",
        "print(whitespace_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VfAvwewZWNi",
        "outputId": "b138aa61-c127-4147-d4c5-532042c72e87"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokenization:\n",
            "['I', \"don't\", 'like', 'bad', 'design!', 'Natural', 'language', 'processing', 'is', 'amazing.', 'Check', 'this', 'out', '@OpenAI', '#NLP', '#AI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "# Punctuation Tokenization\n",
        "punct_tokens = wordpunct_tokenize(text)\n",
        "\n",
        "print(\"\\nPunctuation Tokenization:\")\n",
        "print(punct_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOYXKwZlZk6r",
        "outputId": "d7af9346-1ba0-415e-bde0-72619ed312e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Punctuation Tokenization:\n",
            "['I', 'don', \"'\", 't', 'like', 'bad', 'design', '!', 'Natural', 'language', 'processing', 'is', 'amazing', '.', 'Check', 'this', 'out', '@', 'OpenAI', '#', 'NLP', '#', 'AI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "# Treebank Tokenization\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "treebank_tokens = treebank_tokenizer.tokenize(text)\n",
        "\n",
        "print(\"\\nTreebank Tokenization:\")\n",
        "print(treebank_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkXddm61Zuc-",
        "outputId": "9ebbbb9f-c03a-41a7-ee54-83930952ce3c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treebank Tokenization:\n",
            "['I', 'do', \"n't\", 'like', 'bad', 'design', '!', 'Natural', 'language', 'processing', 'is', 'amazing.', 'Check', 'this', 'out', '@', 'OpenAI', '#', 'NLP', '#', 'AI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# Tweet Tokenization\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
        "\n",
        "print(\"\\nTweet Tokenization:\")\n",
        "print(tweet_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N74YkhXxZ6F3",
        "outputId": "46c0d24f-5e31-40e8-be3d-b480d3abf99f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tweet Tokenization:\n",
            "['I', \"don't\", 'like', 'bad', 'design', '!', 'Natural', 'language', 'processing', 'is', 'amazing', '.', 'Check', 'this', 'out', '@OpenAI', '#NLP', '#AI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "# Define multi-word expressions\n",
        "mwe_tokenizer = MWETokenizer([('natural', 'language'), ('machine', 'learning')])\n",
        "\n",
        "# Apply MWE tokenizer\n",
        "mwe_tokens = mwe_tokenizer.tokenize(text.lower().split())\n",
        "\n",
        "print(\"\\nMWE Tokenization:\")\n",
        "print(mwe_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPt5Vt-haNrt",
        "outputId": "06a8ac51-8014-4957-ba1a-fba5967a7d3c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MWE Tokenization:\n",
            "['i', \"don't\", 'like', 'bad', 'design!', 'natural_language', 'processing', 'is', 'amazing.', 'check', 'this', 'out', '@openai', '#nlp', '#ai']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸŸ¢ PART 2: STEMMING**"
      ],
      "metadata": {
        "id": "EC-PYmFGak8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "words = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\"]\n",
        "\n",
        "print(\"\\nPorter Stemming:\")\n",
        "for word in words:\n",
        "    print(word, \"â†’\", porter.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpTQ1X8QacTZ",
        "outputId": "499cda83-df7c-434b-a60b-f314ba7c4a1e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Porter Stemming:\n",
            "running â†’ run\n",
            "runs â†’ run\n",
            "ran â†’ ran\n",
            "easily â†’ easili\n",
            "fairly â†’ fairli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "\n",
        "print(\"\\nSnowball Stemming:\")\n",
        "for word in words:\n",
        "    print(word, \"â†’\", snowball.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1_vx4tia-hS",
        "outputId": "278f7eaa-255a-4a0f-949c-d66ca48c9c8e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Snowball Stemming:\n",
            "running â†’ run\n",
            "runs â†’ run\n",
            "ran â†’ ran\n",
            "easily â†’ easili\n",
            "fairly â†’ fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸŸ¢ PART 3: LEMMATIZATION**"
      ],
      "metadata": {
        "id": "UpdOo4Whbbrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [\"running\", \"better\", \"cars\", \"went\"]\n",
        "\n",
        "print(\"\\nWordNet Lemmatization:\")\n",
        "for word in words:\n",
        "    print(word, \"â†’\", lemmatizer.lemmatize(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "113sGHcpbKf5",
        "outputId": "fb1b6e0f-2eaf-4d53-fa07-644ab3065cbe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WordNet Lemmatization:\n",
            "running â†’ running\n",
            "better â†’ better\n",
            "cars â†’ car\n",
            "went â†’ went\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verb-based lemmatization\n",
        "print(\"\\nLemmatization with POS='v':\")\n",
        "for word in [\"running\", \"went\"]:\n",
        "    print(word, \"â†’\", lemmatizer.lemmatize(word, pos='v'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu3orl-jbXow",
        "outputId": "169d4be8-cb5c-480d-fbca-3441a02fef41"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatization with POS='v':\n",
            "running â†’ run\n",
            "went â†’ go\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINAL SUMMARY\n",
        "\n",
        "\n",
        "Whitespace Tokenizer: Splits text using spaces only.\n",
        "\n",
        "Punctuation Tokenizer: Splits text at punctuation marks.\n",
        "\n",
        "Treebank Tokenizer: Handles contractions like donâ€™t â†’ do, nâ€™t.\n",
        "\n",
        "Tweet Tokenizer: Designed for social media text (hashtags, mentions, emojis).\n",
        "\n",
        "MWE Tokenizer: Combines multi-word expressions into a single token.\n",
        "\n",
        "Porter Stemmer: Fast rule-based word stemming.\n",
        "\n",
        "Snowball Stemmer: Improved and more accurate stemming.\n",
        "\n",
        "WordNet Lemmatizer: Converts words to their meaningful dictionary form."
      ],
      "metadata": {
        "id": "bAz43v81bxA3"
      }
    }
  ]
}